# CanaryGAN

**Pytorch reimplementation of Pagliarini et al. (2021) "What does the Canary Say? Low-Dimensional GAN Applied to Birdsong"**


## Installation

First, clone the repository on your computer.

We recommend using a virtual environment when using this tool. You may use `virtualenv` or `pyenv` for instance. If using `conda`, pay attention when 
performing the next steps, as some package requirements may differ.

Code should run with `python>=3.9<=3.11`.

### Using pip (recommended)

After cloning the repository and creating a virtual environment, open a terminal and place yourself at the repository root. Activate your virtual environment 
(this step may differ from one virtual environment manager to another).

Now, run:

```bash
pip install -e .
```
This will install `canarygan` along with its dependencies, and add `canarygan` to your `PATH`. You will now be able
to use `canarygan` command line interface.

### Manually from requirements

In some cases, you might want to install requirements manually. **This is required if you need a specific version of Pytorch to run on your machine**.
Package requirements may be found in the `requirements.txt` file, and in the `pyproject.toml` file.

You can install requirements by running the following command within the repository and a virtual environment:

```bash
pip install -r requirements.txt
```

Modify this file, or use `pip` or `conda` is you wish to install packages differently.

You may still try to run `pip install -e .` after this step to add `canarygan` to your `PATH`. **If it does not work, replace all
following invocation of `canarygan` command line interface by `python -m canarygan`.**

### Note on Pytorch

We let `torch` package requirement pretty lose on purpose, but can not ensure this tool will work on any machine and operating system.

This tool was developped using Pytorch 2.0.3, and run on different Linux operating systems, equipped with different hardware. It worked
using Nvidia GPUs (Quadro 4000TX, P100, A100) with CUDA 11.8.


## Command line interface (CLI)

`canarygan` provides a CLI to perform major operations, such as training the GAN and generating sounds.

You can display a short description of the interface by running:

```bash
canarygan --help
```

You should get the following output:

```
❯ canarygan --help               
Usage: canarygan [OPTIONS] COMMAND [ARGS]...

Options:
  --help  Show this message and exit.

Commands:
  build-decoder-dataset  Preprocess dataset for decoder training.
  generate               Generate canary syllables using a trained GAN...
  inception              Compute inception score.
  sample                 Randomly sample GAN latent space and save...
  train-decoders         ESN, kNN and SVM decoders training.
  train-gan              Train a CanaryGAN instance.
  train-inception        Distributed canaryGAN inception scorer training...
  umap                   Make many plots displaying UMAP projections of...``
```

**Note: if you installed `canarygan` manually, you may have to type `python -m canarygan --help` instead.**


## Dataset requirements

Two datasets are required: one to train the GAN and the other to train the decoders.

Both datasets must be WAV files containing 1 second of audio, sampled at 16000Hz at least. If the sampling rate is higher, it will be reduced to 16000Hz automatically.
These 1 second of audio must hold a unique birdsong syllable rendition. Original results were obtained over a dataset of 16 different types of syllables, sampled from 
a single canary individual.

Audio files must be organized in folders named after the canary syllable labels. We recommend that each folder contain the same number of audio samples. In the original
paper, 1000 samples per type of syllable were used to train the GAN and the decoders.

Dataset structure hence resembles this:

```
data_dir/
    |
    |- label_1/
    |   |- ...
    |- label_2/
    |   |- ...
    ...
    |- label_n/
        |- audio_1.wav
        |- audio_2.wav
        |- audio_3.wav
        ...
        |- audio_m.wav
```

**GAN dataset**: GAN training dataset must contain real samples only.

**Decoder dataset**: Decoder training dataset must include GAN training dataset, and append GAN generated songs. In original work, 5 classes of audio were
added: audio samples generated by the GAN at training epoch 15, 30, 45 and last epoch, and white noise samples. These samples were labeled "EARLY15", "EARLY30", 
"EARLY45", "OT" (**O**ver **T**raining"), and "WN" (**W**hite **N**oise). They may also sometimes be refered to alltogether as the "X" class.

## Train the GAN

GAN training loop was implemented using Lightning. Lightning allows from distributed training strategies, using multiple GPUs on multiple compute nodes.
However, this training loop should also work locally on a modern powerful computer.

From your terminal, run `canarygan train-gan --help` to display all options of GAN training.

### Training locally

To train the GAN on a single machine equipped with a single GPU or CPU, you may simply launch:

```bash
canarygan train-gan -d data_dir/ -s save_dir/
```

The `-d` option is used to specify the dataset root directory, which must be structured as previously explained. The `-s` option specifies the save directory,
where all model checkpoints and training logs will be saved during training. If this directory does not exist, it will be created at
runtime. 


### Distributed training

When training in a distributed setup, a bunch of options may be used to attribute compute resources to `canarygan`.

```bash
canarygan train-gan -N 1 -G 2 -c 12 -d data_dir/ -s save_dir/
```

The `-N` option defines the number of computing nodes attributed to this GAN training process. This is only useful when trianing on
a cluster. If using a single machine like your personal computer, keep this value to 1.

The `-G` option defines the number of GPU devices that may be used from training, per node. Here, if we consider training on a machine
equipped with 2 GPUs, we set `-G` to 2.

The `-c` option sets the number of CPU processes attached to the training loop. This is mainly used to leverage data loading and
unloading from and to the GPUs. Here, we launch 12 processes per nodes.

Distributed training may dramatically speed up training. Using 4 Nvidia P100 GPUs on 2 compute nodes, 1000 epochs of training with a
16000 samples dataset would take approximately 30h.

### Logging

By default, logs are written every 100 training steps. Logs may be displayed using Tensorboard:

```bash
tensorboard --logdir save_dir/logs/tensorboard
```

Tensorboard is part of `canarygan` requirements and will be installed by default on your computer when installing `canarygan`.
Logs are also saved as CSV files in `save_dir/logs/csv`.

You may change logging frequency using the `--log-every-n-steps` option.

### Checkpointing

By default, model checkpoints are saved to disk every 15 epochs. You may change checkpointing frequency using the 
`--save-every-n-epochs` option.

Two different kind of checkpoints are being produced: in `save_dir/checkpoints/all`, you may find all training checkpoints
saved every N epochs, while `save_dir/checkpoints/last` saves an image of the last checkpoint saved. 

**Resuming training:**:

Last checkpoint saved may be used to resume training after an interruption, using `--resume` flag:

```bash
canarygan train-gan -d data_dir/ -s save_dir/ --resume
```

**Versioning**

When training several instances and saving them under the same `save_dir` directory, each instance will be
automatically identified by an integer ID, or an ID provided by the user using the `--version` option.

By default, `--version=infer`, meaning that instances will be identified by an integer ID that will be
automatically incremented when launching a new training process, unless using `--resume`, which will resume
training the last trained instance.


## Generate syllables

Once a trained GAN instance is available, syllables can be generated by providing latent vectors 
or randomly sampling the GAN latent space.

### Providing latent vectors

We recommend saving the sampled GAN latent space vectors to disk to increase results reproducibility.
These vectors must be stored in a $n \times d$ matrix saved as a Numpy archive (`.npy`), where
$n$ is the number of samples and $d$ is the dimension of the GAN latent space (3 by default).

To generate these samples, you may use:

```bash
canarygan sample -s save_dir/ -n 10000 -d 3
```

This will create a `.npy` file in `save_dir/` containing 10000 3-dimensional vectors. By default, the vector values
are uniformly distributed between -1 and 1.

You may change the distribution parameters using the `--dist` and `--dist-params` options. Run `canarygan sample --help` 
to access documentation.

### Generate samples

To generate canary syllable samples, run:

```bash
canarygan generate -x path/to/gan.ckpt -n 10000 -s save_dir/ 
```

The `-x` option is required and must point to a GAN checkpoint file obtained through training. 
The `-s` option is also required and provide an endpoint directory for the generated audios. They will
be stored as compressed Numpy archives (`.npz`) files in this directory. These compressed archives contains
the audio signal (in the subfile `x.npy`) and other metadata such as the corresponding latent vector (`z.npy`).
This archive may be loaded using `d = numpy.load(archive_path)`, and subfiles accessed using `d["x"]` or `d["z"]`.

The `-n` option is necessary is you do not wish to provide any pre-computed latent vectors to the script. In that
case, this option specifies the number of latent vectors to randomly sample, and thus the number of generated audios.

If you used `canarygan sample` and wish to generate sounds from precomputed latent vectors, use:

```bash
canarygan generate -x path/to/gan.ckpt -z path/to/vectors.npy -s save_dir/ 
```

The `-z` flag must points towards the Numpy archive storing the latent vectors on disk.


## Train decoders




